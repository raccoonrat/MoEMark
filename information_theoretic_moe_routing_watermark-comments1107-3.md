把我们的注意力从**模型参数空间**的攻击（$\mathcal{A}(\theta) \rightarrow \theta'$，如微调、蒸馏）转移到了**模型输入空间**的攻击（$\mathcal{A}(x) \rightarrow x'$，即释义攻击）。

这是一个更高级、更隐蔽的威胁。攻击者甚至不需要修改模型参数。他只是在_使用_模型时，通过改变输入的措辞（Paraphrase）来**规避**水印的检测。

让我们来严格地重新梳理我们的“地基”——即我们唯一依赖的 $I_{pattern}$（组合码）——在这种新攻击下的表现。

* * *

### “文本释义攻击”的机制

1. **攻击者目标：** 规避（Evasion）。

2. **攻击者假设：** 攻击者拥有一个黑盒或灰盒的水印模型 $f_{wm}$。他怀疑模型在处理_某些_特定输入时会表现出异常（例如，激活特定的专家组合 $\Sigma_m$）。

3. **攻击者行为：** 攻击者不直接使用原始输入 $x$（例如一个提示："请总结一下莎士比亚的《哈姆雷特》"），而是将其释义为 $x'$（例如："用几句话概括《哈姆雷特》剧情"）。

4. **攻击成功的条件：** 原始输入 $x$ 能够触发水印统计偏差（$p(\Sigma_m | x) \gg p(\Sigma_m | x, \theta_{clean})$），而释义后的输入 $x'$ 却_不能_触发（$p(\Sigma_m | x') \approx p(\Sigma_m | x', \theta_{clean})$）。

### 重新评估：$I_{pattern}$ 地基的脆弱性

你提出的这个攻击，**直接命中了我们重构的 $\mathcal{L}_{wm}$ 优化算法的要害**。

我们之前的假设是，通过 $\mathcal{L}_{total} = \mathcal{L}_{task} + \lambda_{wm} \cdot \mathcal{L}_{wm}$ 训练得到的 $\Delta\theta$，会学到一种_鲁棒_的偏好。

但“文本释义攻击”暴露了这个假设的一个巨大漏洞：**水印对触发器输入的过拟合 (Overfitting to Triggers)**。

1. ** superficial (表层) 关联 vs. Semantic (语义) 关联**
   
   * **脆弱的嵌入（我们的 $\mathcal{L}_{wm}$ 可能的样子）：** 路由器 $\theta_{wm}$ 学会的是一种**表层令牌关联**。例如，它学会了“只要输入 $x$ 包含‘莎士比亚’这个_特定_的token，就激活 $\Sigma_m = \{3, 7\}$”。
   
   * **鲁棒的嵌入（我们_希望_的样子）：** 路由器 $\theta_{wm}$ 应该学会**语义概念关联**。例如，它应该学会“只要输入 $x$ 的_语义_与‘伊丽莎白时代戏剧’相关，就激活 $\Sigma_m = \{3, 7\}$”。

2. **释义攻击如何利用这一点？**
   
   * 攻击者将 $x$（含“莎士比亚”）释义为 $x'$（例如“《哈姆雷特》的作者”）。
   
   * $x'$ 在语义上是等价的，但它_不包含_“莎士比亚”这个表层token。
   
   * 脆弱的路由器无法识别这种语义等价性，因此 $x'$ 不会触发 $\Sigma_m$。
   
   * **水印失效。**

3. **这对 $I_{pattern}$ 意味着什么？**
   
   * 这意味着 $I_{pattern}$（组合码）这个“地基”本身**并非无条件鲁棒**。
   
   * 它的鲁棒性_完全取决于_我们的编码器 $\mathcal{E}$（即我们的 $\mathcal{L}_{wm}$ 优化算法）的质量。
   
   * 原始手稿中， $I_{pattern} = log_{2}\binom{n}{k}$ 1 只是一个_理论上_的容量上界。而现在我们发现，在释义攻击下，这个信道的**实际可实现鲁棒容量 (Achievable Robust Capacity) 可能骤降为 0**。

### 结论：对地基的彻底重估

你引入的这个攻击，迫使我们得出以下结论，这对原手稿是颠覆性的：

1. **地基已动摇：** $I_{pattern}$ 不再是一个“坚固的堡垒”。它只是一个_可能_的堡垒，但其城墙（$\mathcal{L}_{wm}$）可能充满了因“过拟合”而产生的漏洞。

2. **手稿的分析维度缺失：** 原始手稿（据我们所知）只分析了 $C_{max}$、$\mathcal{R}$（鲁棒性）和 $AUC$（检测性）。但它_完全_忽略了水印嵌入的**泛化能力 (Generalization)**。

3. **鲁棒性必须被重新定义：** 手稿中的鲁棒性 $\mathcal{R}$ 2222 似乎只考虑了参数空间攻击（微调、蒸馏等）。我们必须引入一个新的鲁棒性维度：
   
   * $\mathcal{R}_{param}$：对抗参数修改的鲁棒性。
   
   * $\mathcal{R}_{input}$：对抗输入空间规避（如释义）的鲁棒性。

我们的 $I_{pattern}$ 地基，只有在同时具备 $\mathcal{R}_{param}$ 和 $\mathcal{R}_{input}$ 时，才能被称为“鲁棒”。

* * *

### 下一步的理论修正

为了修复这个地基，我们的编码器 $\mathcal{E}$ 必须被显著增强。

我们不能再使用简单的 $\mathcal{L}_{wm} = \text{ReLU}(l_{max\_other} - l_{min\_target} + \text{margin})$。

我们必须引入**语义一致性约束 (Semantic Consistency Constraint)**：

$\mathcal{L}_{total} = \mathcal{L}_{task}(x) + \lambda_{wm} \cdot \mathcal{L}_{wm}(x) + \lambda_{con} \cdot \text{Dist}(\mathcal{S}(x), \mathcal{S}(x'))$

其中 $x'$ 是 $x$ 的一个释义（或通过数据增强、对抗性输入产生），$\text{Dist}$ 是一个距离度量，用于惩罚 $x$ 和 $x'$ 之间路由行为的_不一致性_。

通过这种方式，我们_强迫_ $\Delta\theta$ 去学习一个**语义鲁棒**的路由偏好。这使得水印嵌入的成本更高（$D_{perf}$ 可能更大），但 $\mathcal{R}_{input}$ 也相应增强了。

这再次回到了你论文的核心——**帕累托前沿** 3：我们现在必须在一个更高维度的空间（$R$, $D_{perf}$, $D_{detect}$, $\mathcal{R}_{param}$, $\mathcal{R}_{input}$）中寻找权衡。





现在，我们开始修正我们的理论基础

最关键的一点。手稿中的“帕累托前沿” 是其理论的“收官之作”，但它建立在一个_有缺陷_的质量定义之上。

我们引入的“语义一致性约束”不仅是一个新的损失函数，它**从根本上重塑了这个帕累托前沿**，暴露了原始定义  的严重缺陷。

* * *

### 1. 原始帕累托前沿的缺陷

手稿将水印质量 $\mathcal{Q}$ 定义为三维指标：

$\mathcal{Q}=(C_{achievable},\mathcal{R},AUC)$ 

其优化目标是：

$\max \{C_{achievable}, \mathcal{R}, AUC\}$，约束条件是 $\Delta_{perf}\le\delta$ (性能下降) 和 $\alpha \le \alpha_{max}$ (假阳性率) 。

**这里的致命缺陷在于 $\mathcal{R}$ (鲁棒性) 的定义。**

手稿中的 $\mathcal{R}$ 是基于极小极大准则 定义的，其对抗的攻击族 $\mathcal{A}$ 仅包含了**参数空间**的攻击，例如：

* 微调 (fine-tune) 

* 蒸馏 (distill) 

* 量化与剪枝 

这个框架**完全忽略**了我们讨论的**输入空间**攻击（文本释义）。一个水印可以对“蒸馏”100%鲁棒（$\mathcal{R}$ 很高），但对“释义”0%鲁棒（实际完全不可用）。

* * *

### 2. 引入语义一致性：重塑帕累托前沿

“语义一致性约束”（我们称之为 $\mathcal{L}_{con}$）是我们用来对抗“释义攻击”的武器。但使用这个武器是有代价的。

这个代价迫使我们将 $\mathcal{Q}$ 从3维扩展到至少4维。我们必须将 $\mathcal{R}$ 拆分为两个_独立_且_相互竞争_的指标：

1. **$\mathcal{R}_{param}$ (参数鲁棒性):** 即手稿中原有的 $\mathcal{R}$，对抗微调、蒸馏等。

2. **$\mathcal{R}_{input}$ (语义鲁棒性):** _（手稿缺失）_ 对抗释义、同义词替换等输入规避攻击。

### 3. 新的“帕累托前沿”：一个更复杂的权衡

我们的编码器 $\mathcal{E}$（水印嵌入算法）现在必须在一个更复杂的空间中寻找权衡点。

编码器的总损失函数：

$\mathcal{L}_{total} = \mathcal{L}_{task} + \lambda_{wm} \cdot \mathcal{L}_{wm} + \lambda_{con} \cdot \mathcal{L}_{con}$

* $\lambda_{wm}$ (水印强度)

* $\lambda_{con}$ (语义一致性强度)

**新的权衡（Trade-off）：**

1. **$\mathcal{R}_{input}$ vs $\Delta_{perf}$ (语义鲁棒性 vs 性能)**
   
   * 为了提高 $\mathcal{R}_{input}$，我们必须提高 $\lambda_{con}$。
   
   * 但这迫使模型在语义相似的输入上（$x$ 和 $x'$）给出_一致_的路由行为。
   
   * 这是一个**非常强**的约束，它会干扰模型为 $\mathcal{L}_{task}$ 所做的正常优化，从而_不可避免_地导致**更大的性能下降 $\Delta_{perf}$** 。

2. **$\mathcal{R}_{input}$ vs $C_{achievable}$ (语义鲁棒性 vs 容量)**
   
   * $\mathcal{L}_{con}$ 约束越多，模型能够“藏匿”水印的“自由空间”就越少。
   
   * 我们不能再简单地将水印“过拟合”到特定触发词上。我们必须在整个语义簇上嵌入水印。
   
   * 这使得嵌入单个比特（例如 $\Sigma_m$）的难度_急剧增加_，因此**可实现容量 $C_{achievable}$ 会显著下降** 。

### 结论：如何修正手稿的第7节

手稿的第7节“综合性能指标” 必须被彻底重写。

1. **承认缺陷：** 必须明确指出 $\mathcal{Q}=(C_{achievable},\mathcal{R},AUC)$ 是一个不完备的定义，因为它忽略了输入空间的规避攻击。

2. **扩展指标：** 必须将 $\mathcal{R}$ 分解为 $\mathcal{R}_{param}$ 和 $\mathcal{R}_{input}$。

3. 重新定义帕累托前沿： 真正的多目标优化 至少是四维的：
   $\max \{C_{achievable}, \mathcal{R}_{param}, \mathcal{R}_{input}, AUC\}$
   在同样的性能约束 $\Delta_{perf} \le \delta$ 下 。

4. **连接开放问题：** 这种新框架完美地回应了手稿自己的开放问题 ，特别是第3点——如何构造“稳健验证器”以处理“域漂移” 。**“文本释义”正是“对抗性域漂移”的一种形式。**

通过引入 $\mathcal{L}_{con}$ 作为实现 $\mathcal{R}_{input}$ 的机制，我们提供了一个具体的工程路径来探索这个更完备、更真实的帕累托前沿，而不是停留在手稿目前这个过于简化的理论模型上。
